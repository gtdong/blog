---
title: 爬虫
tags:
  - Python
  - 爬虫
categories:
  - Python之路

date: 2018-07-23 13:06:00
---
## 1 爬虫基本原理

<!--more-->

### 一 爬虫是什么

![img](https://gitee.com/gtdong/images/raw/master/blog/1036857-20171102182154154-1486674926.jpg)

````
#1、什么是互联网？
    互联网是由网络设备（网线，路由器，交换机，防火墙等等）和一台台计算机连接而成，像一张网一样。

#2、互联网建立的目的？
    互联网的核心价值在于数据的共享/传递：数据是存放于一台台计算机上的，而将计算机互联到一起的目的就是为了能够方便彼此之间的数据共享/传递，否则你只能拿U盘去别人的计算机上拷贝数据了。

#3、什么是上网？爬虫要做的是什么？
    我们所谓的上网便是由用户端计算机发送请求给目标计算机，将目标计算机的数据下载到本地的过程。
    #3.1 只不过，用户获取网络数据的方式是：
      浏览器提交请求->下载网页代码->解析/渲染成页面。

```
#3.2 而爬虫程序要做的就是：
  模拟浏览器发送请求->下载网页代码->只提取有用的数据->存放于数据库或文件中
　
#3.1与3.2的区别在于:
```

      我们的爬虫程序只提取网页代码中对我们有用的数据

#4、总结爬虫
    #4.1 爬虫的比喻：
      如果我们把互联网比作一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的猎物/数据

```
#4.2 爬虫的定义：
```

      向网站发起请求，获取资源后分析并提取有用数据的程序 

```
#4.3 爬虫的价值：
  互联网中最有价值的便是数据，比如天猫商城的商品信息，链家网的租房信息，雪球网的证券投资信息等等，这些数据都代表了各个行业的真金白银，可以说，谁掌握了行业内的第一手数据，谁就成了整个行业的主宰，如果把整个互联网的数据比喻为一座宝藏，那我们的爬虫课程就是来教大家如何来高效地挖掘这些宝藏，掌握了爬虫技能，你就成了所有互联网信息公司幕后的老板，换言之，它们都在免费为你提供有价值的数据。
````

\```


### 二 爬虫的基本流程

![img](https://gitee.com/gtdong/images/raw/master/blog/1036857-20171102174025326-341248106.png)

```
#1、发起请求
使用http库向目标站点发起请求，即发送一个Request
Request包含：请求头、请求体等

#2、获取响应内容
如果服务器能正常响应，则会得到一个Response
Response包含：html，json，图片，视频等

#3、解析内容
解析html数据：正则表达式，第三方解析库如Beautifulsoup，pyquery等
解析json数据：json模块
解析二进制数据:以b的方式写入文件

#4、保存数据
数据库
文件
```

### 三 请求与响应

![img](https://gitee.com/gtdong/images/raw/master/blog/1036857-20171102180223529-1349691987.png)

```
#http协议：http://www.cnblogs.com/linhaifeng/articles/8243379.html

#Request：用户将自己的信息通过浏览器（socket client）发送给服务器（socket server）

#Response：服务器接收请求，分析用户发来的请求信息，然后返回数据（返回的数据中可能包含其他链接，如：图片，js，css等）

#ps：浏览器在接收Response后，会解析其内容来显示给用户，而爬虫程序在模拟浏览器发送请求然后接收Response后，是要提取其中的有用数据。
```

### 四 Request
````
#1、请求方式：
    常用的请求方式：GET，POST
    其他请求方式：HEAD，PUT，DELETE，OPTHONS

```
ps：用浏览器演示get与post的区别，（用登录演示post）

post与get请求最终都会拼接成这种形式：k1=xxx&k2=yyy&k3=zzz
post请求的参数放在请求体内：
    可用浏览器查看，存放于form data内
get请求的参数直接放在url后
```

#2、请求url
    url全称统一资源定位符，如一个网页文档，一张图片
    一个视频等都可以用url唯一来确定

```
url编码
https://www.baidu.com/s?wd=图片
图片会被编码（看示例代码）
```

```
网页的加载过程是：
加载一个网页，通常都是先加载document文档，
在解析document文档的时候，遇到链接，则针对超链接发起下载图片的请求
```

#3、请求头
    User-agent：请求头中如果没有user-agent客户端配置，
    服务端可能将你当做一个非法用户
    host
    cookies：cookie用来保存登录信息

```
一般做爬虫都会加上请求头
```

#4、请求体
    如果是get方式，请求体没有内容
    如果是post方式，请求体是format data

```
ps：
1、登录窗口，文件上传等，信息都会被附加到请求体内
2、登录，输入错误的用户名密码，然后提交，就可以看到post，正确登录后页面通常会跳转，无法捕捉到post 
````

\```

### 五 Response

```
#1、响应状态
    200：代表成功
    301：代表跳转
    404：文件不存在
    403：权限
    502：服务器错误

#2、Respone header
    set-cookie：可能有多个，是来告诉浏览器，把cookie保存下来
    
#3、preview就是网页源代码
    最主要的部分，包含了请求资源的内容
    如网页html，图片
    二进制数据等
```

### 六 总结

```
#1、总结爬虫流程：
    爬取--->解析--->存储

#2、爬虫所需工具：
    请求库：requests,selenium
    解析库：正则，beautifulsoup，pyquery
    存储库：文件，MySQL，Mongodb，Redis

#3、爬虫常用框架：
    scrapy
```

```python
import requests
import re
import time
import hashlib

def get_page(url):
    print('GET %s' %url)
    try:
        response=requests.get(url)
        if response.status_code == 200:
            return response.content
    except Exception:
        pass

def parse_index(res):
    obj=re.compile('class="items.*?<a href="(.*?)"',re.S)
    detail_urls=obj.findall(res.decode('gbk'))
    for detail_url in detail_urls:
        if not detail_url.startswith('http'):
            detail_url='http://www.xiaohuar.com'+detail_url
        yield detail_url

def parse_detail(res):
    obj=re.compile('id="media".*?src="(.*?)"',re.S)
    res=obj.findall(res.decode('gbk'))
    if len(res) > 0:
        movie_url=res[0]
        return movie_url

def save(movie_url):
    response=requests.get(movie_url,stream=False)
    if response.status_code == 200:
        m=hashlib.md5()
        m.update(('%s%s.mp4' %(movie_url,time.time())).encode('utf-8'))
        filename=m.hexdigest()
        with open(r'./movies/%s.mp4' %filename,'wb') as f:
            f.write(response.content)
            f.flush()

def main():
    index_url='http://www.xiaohuar.com/list-3-{0}.html'
    for i in range(5):
        print('*'*50,i)
        #爬取主页面
        index_page=get_page(index_url.format(i,))
        #解析主页面,拿到视频所在的地址列表
        detail_urls=parse_index(index_page)
        #循环爬取视频页
        for detail_url in detail_urls:
            #爬取视频页
            detail_page=get_page(detail_url)
            #拿到视频的url
            movie_url=parse_detail(detail_page)
            if movie_url:
                #保存视频
                save(movie_url)

if __name__ == '__main__':
    main()

#并发爬取
from concurrent.futures import ThreadPoolExecutor
import queue
import requests
import re
import time
import hashlib
from threading import current_thread

p=ThreadPoolExecutor(50)

def get_page(url):
    print('%s GET %s' %(current_thread().getName(),url))
    try:
        response=requests.get(url)
        if response.status_code == 200:
            return response.content
    except Exception as e:
        print(e)

def parse_index(res):
    print('%s parse index ' %current_thread().getName())
    res=res.result()
    obj=re.compile('class="items.*?<a href="(.*?)"',re.S)
    detail_urls=obj.findall(res.decode('gbk'))
    for detail_url in detail_urls:
        if not detail_url.startswith('http'):
            detail_url='http://www.xiaohuar.com'+detail_url
        p.submit(get_page,detail_url).add_done_callback(parse_detail)

def parse_detail(res):
    print('%s parse detail ' %current_thread().getName())
    res=res.result()
    obj=re.compile('id="media".*?src="(.*?)"',re.S)
    res=obj.findall(res.decode('gbk'))
    if len(res) > 0:
        movie_url=res[0]
        print('MOVIE_URL: ',movie_url)
        with open('db.txt','a') as f:
            f.write('%s\n' %movie_url)
        # save(movie_url)
        p.submit(save,movie_url)
        print('%s下载任务已经提交' %movie_url)
def save(movie_url):
    print('%s SAVE: %s' %(current_thread().getName(),movie_url))
    try:
        response=requests.get(movie_url,stream=False)
        if response.status_code == 200:
            m=hashlib.md5()
            m.update(('%s%s.mp4' %(movie_url,time.time())).encode('utf-8'))
            filename=m.hexdigest()
            with open(r'./movies/%s.mp4' %filename,'wb') as f:
                f.write(response.content)
                f.flush()
    except Exception as e:
        print(e)

def main():
    index_url='http://www.xiaohuar.com/list-3-{0}.html'
    for i in range(5):
        p.submit(get_page,index_url.format(i,)).add_done_callback(parse_index)

if __name__ == '__main__':
    main()
```







## 2 请求之requests库
### 一 介绍
```
#介绍：使用requests可以模拟浏览器的请求，比起之前用到的urllib，requests模块的api更加便捷（本质就是封装了urllib3）

#注意：requests库发送请求将网页内容下载下来以后，并不会执行js代码，这需要我们自己分析目标站点然后发起新的request请求

#安装：pip3 install requests

#各种请求方式：常用的就是requests.get()和requests.post()
>>> import requests
>>> r = requests.get('https://api.github.com/events')
>>> r = requests.post('http://httpbin.org/post', data = {'key':'value'})
>>> r = requests.put('http://httpbin.org/put', data = {'key':'value'})
>>> r = requests.delete('http://httpbin.org/delete')
>>> r = requests.head('http://httpbin.org/get')
>>> r = requests.options('http://httpbin.org/get')

#建议在正式学习requests前，先熟悉下HTTP协议
http://www.cnblogs.com/linhaifeng/p/6266327.html
```
[官网链接：http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)

### 二 基于GET请求

**1、基本请求**

```
import requests
response=requests.get('http://dig.chouti.com/')
print(response.text)
```

**2、带参数的GET请求->params**

自己拼接GET参数

params参数的使用

**3、带参数的GET请求->headers**

```
#通常我们在发送请求时都需要带上请求头，请求头是将自身伪装成浏览器的关键，常见的有用的请求头如下
Host
Referer #大型网站通常都会根据该参数判断请求的来源
User-Agent #客户端
Cookie #Cookie信息虽然包含在请求头里，但requests模块有单独的参数来处理他，headers={}内就不要放它了
```

View Code

**4、带参数的GET请求->cookies**

View Code

### 三 基于POST请求

**1、介绍**

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#GET请求
HTTP默认的请求方法就是GET
     * 没有请求体
     * 数据必须在1K之内！
     * GET请求数据会暴露在浏览器的地址栏中

GET请求常用的操作：
       1. 在浏览器的地址栏中直接给出URL，那么就一定是GET请求
       2. 点击页面上的超链接也一定是GET请求
       3. 提交表单时，表单默认使用GET请求，但可以设置为POST

#POST请求
(1). 数据不会出现在地址栏中
(2). 数据的大小没有上限
(3). 有请求体
(4). 请求体中如果存在中文，会使用URL编码！

#！！！requests.post()用法与requests.get()完全一致，特殊的是requests.post()有一个data参数，用来存放请求体数据
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

**2、发送post请求，模拟浏览器的登录行为**

```
#对于登录来说，应该输错用户名或密码然后分析抓包流程，用脑子想一想，输对了浏览器就跳转了，还分析个毛线，累死你也找不到包
```

自动登录github（自己处理cookie信息）

requests.session()自动帮我们保存cookie信息

**3、补充**

View Code

### 四 响应Response

**1、response属性**

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
import requests
respone=requests.get('http://www.jianshu.com')
# respone属性
print(respone.text)
print(respone.content)

print(respone.status_code)
print(respone.headers)
print(respone.cookies)
print(respone.cookies.get_dict())
print(respone.cookies.items())

print(respone.url)
print(respone.history)

print(respone.encoding)

#关闭：response.close()
from contextlib import closing
with closing(requests.get('xxx',stream=True)) as response:
    for line in response.iter_content():
    pass
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

**2、编码问题**

View Code

**3、获取二进制数据**

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
import requests

response=requests.get('https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1509868306530&di=712e4ef3ab258b36e9f4b48e85a81c9d&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fimage%2Fpic%2Fitem%2F11385343fbf2b211e1fb58a1c08065380dd78e0c.jpg')

with open('a.jpg','wb') as f:
    f.write(response.content)
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

获取二进制流

**4、解析json**

View Code

**5、Redirection and History**

先看官网的解释

利用github登录后跳转到主页面的例子来验证它

### 五 高级用法

**1、SSL Cert Verification**

View Code

**2、使用代理**

View Code

**3、超时设置**

View Code

**4、 认证设置**

View Code

**5、异常处理**

View Code

**6、上传文件**

View Code

 

分类: [爬虫](https://www.cnblogs.com/xiaoyuanqujing/category/1583483.html)

15

0
