---
title:  K8s-RBAC授权
tags:
  - K8s
  - 云原生
categories:
  - K8s
date: 2022-05-05 16:45:00
---

**实验环境:**
Prometheus+grafana+alertmanager 安装在 k8s 集群，k8s 环境如下:

**k8s 集群:**

​	k8s 的控制节点

​	ip:192.168.40.180

​	主机名:xianchaomaster1

​    配置:4vCPU/4Gi 内存

**k8s 的工作节点:**

​	ip:192.168.40.181 

   主机名:xianchaonode1

   配置:4vCPU/4Gi 内存

## 1、Prometheus 介绍?

 Prometheus 是一个开源的系统监控和报警系统，现在已经加入到 CNCF 基金会，成为继 k8s 之后第二

个在 CNCF 托管的项目，在 kubernetes 容器管理系统中，通常会搭配 prometheus 进行监控，同时也支持 多种 exporter 采集数据，还支持 pushgateway 进行数据上报，Prometheus 性能足够支撑上万台规模的集 群。

## 2、Prometheus 特点?

 1.多维度数据模型
 每一个时间序列数据都由 metric 度量指标名称和它的标签 labels 键值对集合唯一确定:
 这个 metric 度量指标名称指定监控目标系统的测量特征(如:http_requests_total- 接收 http 请

求的总计数)。labels 开启了 Prometheus 的多维数据模型:对于相同的度量名称，通过不同标签列表的 结合, 会形成特定的度量维度实例。(例如:所有包含度量名称为/api/tracks 的 http 请求，打上 method=POST 的标签，则形成了具体的 http 请求)。这个查询语言在这些度量和标签列表的基础上进行过 滤和聚合。改变任何度量上的任何标签值，则会形成新的时间序列图。

2.灵活的查询语言(PromQL)
 可以对采集的 metrics 指标进行加法，乘法，连接等操作; 

3.可以直接在本地部署，不依赖其他分布式存储; 

4.通过基于 HTTP 的 pull 方式采集时序数据;
 5.可以通过中间网关 pushgateway 的方式把时间序列数据推送到 prometheus server 端; 6.可通过服务发现或者静态配置来发现目标服务对象(targets)。
 7.有多种可视化图像界面，如 Grafana 等。
 8.高效的存储，每个采样数据占 3.5 bytes 左右，300 万的时间序列，30s 间隔，保留 60 天，消耗

磁盘大概 200G。
 9.做高可用，可以对数据做异地备份，联邦集群，部署多套 prometheus，pushgateway 上报数据

### 2.1 样本

 在时间序列中的每一个点称为一个样本(sample)，样本由以下三部分组成: 1、指标(metric):指标名称和描述当前样本特征的 labelsets; 2、时间戳(timestamp):一个精确到毫秒的时间戳; 3、样本值(value): 一个 folat64 的浮点型数据表示当前样本的值。

表示方式:
 通过如下表达方式表示指定指标名称和指定标签集合的时间序列:
 <metric name>{<label name>=<label value>, ...}
 例如，指标名称为 api_http_requests_total，标签为 method="POST" 和 handler="/messages"

的时间序列可以表示为:
 api_http_requests_total{method="POST", handler="/messages"}

3、Prometheus 组件介绍
 1.Prometheus Server: 用于收集和存储时间序列数据。
 2.Client Library: 客户端库，检测应用程序代码，当 Prometheus 抓取实例的 HTTP 端点时，客户

端库会将所有跟踪的 metrics 指标的当前状态发送到 prometheus server 端。
 3.Exporters: prometheus 支持多种 exporter，通过 exporter 可以采集 metrics 数据，然后发送到

prometheus server 端，所有向 promtheus server 提供监控数据的程序都可以被称为 exporter 4.Alertmanager: 从 Prometheus server 端接收到 alerts 后，会进行去重，分组，并路由到相应

的接收方，发出报警，常见的接收方式有:电子邮件，微信，钉钉, slack 等。 5.Grafana:监控仪表盘，可视化监控数据
 6.pushgateway: 各个目标主机可上报数据到 pushgateway，然后 prometheus server 统一从

pushgateway 拉取数据。

![image-20220524212315659](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524212315659.png)

从上图可发现，Prometheus 整个生态圈组成主要包括 prometheus server，Exporter， pushgateway，alertmanager，grafana，Web ui 界面，Prometheus server 由三个部分组成， Retrieval，Storage，PromQL

1.Retrieval 负责在活跃的 target 主机上抓取监控指标数据 2.Storage 存储主要是把采集到的数据存储到磁盘中 3.PromQL 是 Prometheus 提供的查询语言模块。

## 4、Prometheus 工作流程

 1.Prometheus server 可定期从活跃的(up)目标主机上(target)拉取监控指标数据，目标主机的

监控数据可通过配置静态 job 或者服务发现的方式被 prometheus server 采集到，这种方式默认的 pull 方式拉取指标;也可通过 pushgateway 把采集的数据上报到 prometheus server 中;还可通过一些组件 自带的 exporter 采集相应组件的数据;

2.Prometheus server 把采集到的监控指标数据保存到本地磁盘或者数据库;

3.Prometheus 采集的监控指标数据按时间序列存储，通过配置报警规则，把触发的报警发送到 alertmanager

4.Alertmanager 通过配置报警接收方，发送报警到邮件，微信或者钉钉等 5.Prometheus 自带的 web ui 界面提供 PromQL 查询语言，可查询监控数据 6.Grafana 可接入 prometheus 数据源，把监控数据以图形化形式展示出

4、Prometheus 和 zabbix 对比分析

![image-20220524212449495](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524212449495.png)

## 5、Prometheus 的几种部署模式 

### 5.1 基本高可用模式

![image-20220524212542252](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524212542252.png)

基本的 HA 模式只能确保 Promthues 服务的可用性问题，但是不解决 Prometheus Server 之间的数据 一致性问题以及持久化问题(数据丢失后无法恢复)，也无法进行动态的扩展。因此这种部署方式适合监 控规模不大，Promthues Server 也不会频繁发生迁移的情况，并且只需要保存短周期监控数据的场景。

### 5.2 基本高可用+远程存储

![image-20220524212559071](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524212559071.png)

在解决了 Promthues 服务可用性的基础上，同时确保了数据的持久化，当 Promthues Server 发生宕 机或者数据丢失的情况下，可以快速的恢复。 同时 Promthues Server 可能很好的进行迁移。因此，该 方案适用于用户监控规模不大，但是希望能够将监控数据持久化，同时能够确保 Promthues Server 的可 迁移性的场景。

### 5.3 基本 HA + 远程存储 + 联邦集群方案

![image-20220524212622268](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524212622268.png)

Promthues 的性能瓶颈主要在于大量的采集任务，因此用户需要利用 Prometheus 联邦集群的特性， 将不同类型的采集任务划分到不同的 Promthues 子服务中，从而实现功能分区。例如一个 Promthues Server 负责采集基础设施相关的监控指标，另外一个 Prometheus Server 负责采集应用监控指标。再有 上层 Prometheus Server 实现对数据的汇聚。

## 6、Prometheus 的四种数据类型 

### 6.1 Counter

Counter 是计数器类型:
 1、Counter 用于累计值，例如记录请求次数、任务完成数、错误发生次数。
 2、一直增加，不会减少。
 3、重启进程后，会被重置。 例如:http_response_total{method="GET",endpoint="/api/tracks"} 100

http_response_total{method="GET",endpoint="/api/tracks"} 160

Counter 类型数据可以让用户方便的了解事件产生的速率的变化，在 PromQL 内置的相关操作函数可 以提供相应的分析，比如以 HTTP 应用请求量来进行说明:

1、通过 rate()函数获取 HTTP 请求量的增长率 rate(http_requests_total[5m])

 2、查询当前系统中，访问量前 10 的 HTTP 地址 topk(10, http_requests_total)

### 6.2 Gauge

 Gauge 是测量器类型:
 1、Gauge 是常规数值，例如温度变化、内存使用变化。
 2、可变大，可变小。
 3、重启进程后，会被重置

例如:

memory_usage_bytes{host="master-01"} 100 memory_usage_bytes{host="master-01"} 30 memory_usage_bytes{host="master-01"} 50 memory_usage_bytes{host="master-01"} 80

对于 Gauge 类型的监控指标，通过 PromQL 内置函数 delta() 可以获取样本在一段时间内的变化 情况，例如，计算 CPU 温度在两小时内的差异:

dalta(cpu_temp_celsius{host="zeus"}[2h])

你还可以通过 PromQL 内置函数 predict_linear() 基于简单线性回归的方式，对样本数据的变化趋 势做出预测。例如，基于 2 小时的样本数据，来预测主机可用磁盘空间在 4 个小时之后的剩余情况:

predict_linear(node_filesystem_free{job="node"}[2h], 4 * 3600) < 0

### 6.3 histogram

 histogram 是柱状图，在 Prometheus 系统的查询语言中，有三种作用: 

1、在一段时间范围内对数据进行采样(通常是请求持续时间或响应大小等)，并将其计入可配置的

存储桶(bucket)中. 后续可通过指定区间筛选样本，也可以统计样本总数，最后一般将数据展示为直 方图。

2、对每个采样点值累计和(sum) 3、对采样点的次数累计和(count)

度量指标名称: [basename]_上面三类的作用度量指标名称 1、[basename]_bucket{le="上边界"}, 这个值为小于等于上边界的所有采样点数量 2、[basename]_sum
 3、[basename]_count

小结:如果定义一个度量类型为 Histogram，则 Prometheus 会自动生成三个对应的指标 6.3.1 为什需要用 histogram 柱状图?

在大多数情况下人们都倾向于使用某些量化指标的平均值，例如 CPU 的平均使用率、页面的平均响 应时间。这种方式的问题很明显，以系统 API 调用的平均响应时间为例:如果大多数 API 请求都维持 在 100ms 的响应时间范围内，而个别请求的响应时间需要 5s，那么就会导致某些 WEB 页面的响应时间 落到中位数的情况，而这种现象被称为长尾问题。

为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计 延迟在 0~10ms 之间的请求数有多少，而 10~20ms 之间的请求数又有多少。通过这种方式可以快速分析 系统慢的原因。Histogram 和 Summary 都是为了能够解决这样问题的存在，通过 Histogram 和 Summary 类型的监控指标，我们可以快速了解监控样本的分布情况。

Histogram 类型的样本会提供三种指标(假设指标名称为 <basename>):

样本的值分布在 bucket 中的数量，命名为 <basename>_bucket{le="<上边界>"}。解释的更通俗易 懂一点，这个值表示指标值小于等于上边界的所有样本数量。

1、在总共 2 次请求当中。http 请求响应时间 <=0.005 秒 的请求次数为 0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.005",} 0.0

2、在总共 2 次请求当中。http 请求响应时间 <=0.01 秒 的请求次数为 0 io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code="200 ",le="0.01",} 0.0

3、在总共 2 次请求当中。http 请求响应时间 <=0.025 秒 的请求次数为 0 io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code="200 ",le="0.025",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.05",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.075",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.1",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.25",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.5",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="0.75",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="1.0",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="2.5",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="5.0",} 0.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="7.5",} 2.0

4、在总共 2 次请求当中。http 请求响应时间 <=10 秒 的请求次数为 2

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="10.0",} 2.0

io_namespace_http_requests_latency_seconds_histogram_bucket{path="/",method="GET",code= "200",le="+Inf",} 2.0

所有样本值的大小总和，命名为 <basename>_sum。
 5、实际含义: 发生的 2 次 http 请求总的响应时间为 13.107670803000001 秒 io_namespace_http_requests_latency_seconds_histogram_sum{path="/",method="GET",code="20

0",} 13.107670803000001

样本总数，命名为 <basename>_count。值和 <basename>_bucket{le="+Inf"} 相同。 6、实际含义: 当前一共发生了 2 次 http 请求 io_namespace_http_requests_latency_seconds_histogram_count{path="/",method="GET",code="200",} 2.0

注意:

bucket 可以理解为是对数据指标值域的一个划分，划分的依据应该基于数据值的分布。注意后面的 采样点是包含前面的采样点的，假设 xxx_bucket{...,le="0.01"} 的值为 10，而 xxx_bucket{...,le="0.05"} 的值为 30，那么意味着这 30 个采样点中，有 10 个是小于 0.01s 的，其 余 20 个采样点的响应时间是介于 0.01s 和 0.05s 之间的。

可以通过 histogram_quantile() 函数来计算 Histogram 类型样本的分位数。分位数可能不太好理 解，你可以理解为分割数据的点。我举个例子，假设样本的 9 分位数(quantile=0.9)的值为 x，即表 示小于 x 的采样值的数量占总体采样值的 90%。Histogram 还可以用来计算应用性能指标值(Apdex score)。

### 6.4 summary

 与 Histogram 类型类似，用于表示一段时间内的数据采样结果(通常是请求持续时间或响应大小

等)，但它直接存储了分位数(通过客户端计算，然后展示出来)，而不是通过区间来计算。它也有三种 作用:

​	1、对于每个采样点进行统计，并形成分位图。(如:正态分布一样，统计低于 60 分不及格的同 学比例，统计低于 80 分的同学比例，统计低于 95 分的同学比例)
​	2、统计班上所有同学的总成绩(sum) 
​	3、统计班上同学的考试总人数(count)

带有度量指标的[basename]的 summary 在抓取时间序列数据有如命名。

​	 1、观察时间的φ-quantiles (0 ≤ φ ≤ 1), 显示为[basename]{分位数="[φ]"} 
 	2、[basename]_sum， 是指所有观察值的总和
​	 3、[basename]_count, 是指已观察到的事件计数值

样本值的分位数分布情况，命名为 <basename>{quantile="<φ>"}。

 1、含义:这 12 次 http 请求中有 50% 的请求响应时间是 3.052404983s

io_namespace_http_requests_latency_seconds_summary{path="/",method="GET",code="200",quantil e="0.5",} 3.052404983

2、含义:这 12 次 http 请求中有 90% 的请求响应时间是 8.003261666s

io_namespace_http_requests_latency_seconds_summary{path="/",method="GET",code="200",quantil e="0.9",} 8.003261666

所有样本值的大小总和，命名为 <basename>_sum。

 1、含义:这 12 次 http 请求的总响应时间为 51.029495508s

io_namespace_http_requests_latency_seconds_summary_sum{path="/",method="GET",code="200",} 51.029495508

样本总数，命名为 <basename>_count。

 1、含义:当前一共发生了 12 次 http 请求

io_namespace_http_requests_latency_seconds_summary_count{path="/",method="GET",code="200",} 12.0

​	现在可以总结一下 Histogram 与 Summary 的异同:
 	它们都包含了 <basename>_sum 和 <basename>_count 指标
 Histogram 需要通过 <basename>_bucket 来计算分位数，而 Summary 则直接存储了分位数的值。

prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.5"} 0.012352463 prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.9"} 0.014458005 prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.99"} 0.017316173 prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002 prometheus_tsdb_wal_fsync_duration_seconds_count 216

从上面的样本中可以得知当前 Promtheus Server 进行 wal_fsync 操作的总次数为 216 次，耗时 2.888716127000002s。其中中位数(quantile=0.5)的耗时为 0.012352463，9 分位数(quantile=0.9) 的耗时为 0.014458005s。

## 7、Prometheus 能监控什么?

* Databases
* Hardware related
* Messaging systems
* Storage
* HTTP
* APIs
* Logging
* Other monitoring systems Miscellaneous
* Software exposing Prometheus metrics

### 7.1 DATABASES-数据库

* Aerospike exporter
*  ClickHouse exporter
*  Consul exporter (official) 
*  Couchbase exporter
*  CouchDB exporter
*  ElasticSearch exporter
*  EventStore exporter
*  Memcached exporter (official)
*  MongoDB exporter
*  MSSQL server exporter
*  MySQL server exporter (official) • OpenTSDB Exporter
* Oracle DB Exporter
*  PgBouncer exporter 
* PostgreSQL exporter 
* ProxySQL exporter
*  RavenDB exporter
*  Redis exporter
*  RethinkDB exporter
*  SQL exporter
*  Tarantool metric library 
*  Twemproxy

### 7.2 HARDWARE RELATED-硬件相关

* apcupsd exporter 

* Collins exporter
* IBM Z HMC exporter
* IoT Edison exporter
* IPMI exporter
* knxd exporter
* Netgear Cable Modem Exporter
* Node/system metrics exporter (official)
* NVIDIA GPU exporter
* ProSAFE exporter
* Ubiquiti UniFi exporter

### 7.3 Messaging systems-消息服务

* Beanstalkd exporter
* Gearman exporter 
* Kafka exporter
* NATS exporter
* NSQ exporter
* Mirth Connect exporter
* MQTT blackbox exporter
* RabbitMQ exporter
* RabbitMQ Management Plugin exporter

### 7.4 Storage-存储

* Ceph exporter
* Ceph RADOSGW exporter 
* Gluster exporter
* Hadoop HDFS FSImage exporter • Lustre exporter
* ScaleIO exporter

### 7.5 http-网站服务

* Apache exporter

### 7.6 API

* HAProxy exporter (official) 
* Nginx metric library
* Nginx VTS exporter
* Passenger exporter
* Squid exporter
* Tinyproxy exporter 
* Varnish exporter
* WebDriver exporter
* AWS ECS exporter
* AWS Health exporter
* AWS SQS exporter
* Cloudflare exporter
* DigitalOcean exporter
* Docker Cloud exporter
* Docker Hub exporter
* GitHub exporter
*  InstaClustr exporter
*  Mozilla Observatory exporter
*  OpenWeatherMap exporte
*  Pagespeed exporter
*  Rancher exporter
*  Speedtest exporter

### 7.7 Logging-日志

* Fluentd exporter
*  Google's mtail log data extractor
* Grok exporter

### 7.8 Other monitoring systems

 • Akamai Cloudmonitor exporter

• Alibaba Cloudmonitor exporter
 • AWS CloudWatch exporter (official) • Cloud Foundry Firehose exporter
 • Collectd exporter (official)

• Google Stackdriver exporter • Graphite exporter (official) • Heka dashboard exporter
 • Heka exporter
 • InfluxDB exporter (official) • JavaMelody exporter
 • JMX exporter (official)
 • Munin exporter
 • Nagios / Naemon exporter
 • New Relic exporter
 • NRPE exporter
 • Osquery exporter
 • OTC CloudEye exporter
 • Pingdom exporter
 • scollector exporter
 • Sensu exporter
 • SNMP exporter (official)
 • StatsD exporter (official)

### 7.9 Miscellaneous-其他

* ACT Fibernet Exporter
* Bamboo exporter
* BIG-IP exporter
* BIND exporter
* Bitbucket exporter
* Blackbox exporter (official) 
* BOSH exporter
* cAdvisor
* Cachet exporter
* ccache exporter
* Confluence exporter
* Dovecot exporter
* eBPF exporter
* Ethereum Client exporter
* jenkins exporter
* JIRA exporter
* Kannel exporter
* Kemp LoadBalancer exporter
* Kibana Exporter
* Meteor JS web framework exporter
* Minecraft exporter module
*  PHP-FPM exporter
*  PowerDNS exporter
*  Presto exporter
*  Process exporter
*  rTorrent exporter
*  SABnzbd exporter
* Script exporter
*  Shield exporter
*  SMTP/Maildir MDA blackbox prober 
*  SoftEther exporter
*  Transmission exporter
*   Unbound exporter
*   Xen exporter

### 7.10 Software exposing Prometheus metrics-Prometheus 度量指标 

* App Connect Enterprise
* Ballerina
* Ceph
* Collectd
* Concourse
* CRG Roller Derby Scoreboard (direct) 
* Docker Daemon
* Doorman (direct) 
* Etcd (direct)
* Flink
* FreeBSD Kernel
* Grafana
* JavaMelody
* Kubernetes (direct) 
* Linkerd

## 8、Prometheus 对 kubernetes 的监控

 对于 Kubernetes 而言，我们可以把当中所有的资源分为几类:

• 基础设施层(Node):集群节点，为整个集群和应用提供运行时资源
 • 容器基础设施(Container):为应用提供运行时环境
 • 用户应用(Pod):Pod中会包含一组容器，它们一起工作，并且对外提供一个(或者一

组)功能
 • 内部服务负载均衡(Service):在集群内，通过Service在集群暴露应用功能，集群内

应用和应用之间访问时提供内部的负载均衡
 • 外部访问入口(Ingress):通过Ingress提供集群外的访问入口，从而可以使外部客户

端能够访问到部署在 Kubernetes 集群内的服务

因此，如果要构建一个完整的监控体系，我们应该考虑，以下 5 个方面:
 • 集群节点状态监控:从集群中各节点的kubelet服务获取节点的基本运行状态;
 • 集群节点资源用量监控:通过Daemonset的形式在集群中各个节点部署NodeExporter

​    采集节点的资源使用情况;

• 节点中运行的容器监控:通过各个节点中kubelet内置的cAdvisor中获取个节点中所有 容器的运行状态和资源使用情况;

• 如果在集群中部署的应用程序本身内置了对Prometheus的监控支持，那么我们还应该找 到相应的 Pod 实例，并从该 Pod 实例中获取其内部运行状态的监控指标。

• 对k8s本身的组件做监控:apiserver、scheduler、controller-manager、kubelet、 kube-proxy

## 9、node-exporter 组件安装和配置 机器规划:

我的实验环境使用的 k8s 集群是一个 master 节点和一个 node 节点 master 节点的机器 ip 是 192.168.40.180，主机名是 xianchaomaster1 node 节点的机器 ip 是 192.168.40.181，主机名是 xianchaonode1

### 9.1 node-exporter 介绍?

 node-exporter 可以采集机器(物理机、虚拟机、云主机等)的监控指标数据，能够采集到的指标包

括 CPU, 内存，磁盘，网络，文件数等信息。

### 9.2 安装 node-exporter

```shell
 [root@xianchaomaster1 ~]# kubectl create ns monitor-sa
```

 把课件里的 node-exporter.tar.gz 镜像压缩包上传到 k8s 的各个节点，手动解压: 

```shell
[root@xianchaomaster1 ~]# docker load -i node-exporter.tar.gz
[root@xianchaonode1 ~]# docker load -i node-exporter.tar.gz node-export.yaml 文件在课件，可上传到自己 k8s 的控制节点 
xianchaomaster1: cat node-export.yaml
```

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitor-sa
  labels:
    name: node-exporter
spec:
  selector:
    matchLabels:
     name: node-exporter
  template:
    metadata:
      labels:
        name: node-exporter
    spec:
      hostPID: true
      hostIPC: true
      hostNetwork: true
      # hostNetwork、hostIPC、hostPID都为True时，表示这个Pod里的所有容器，会直接使用宿主机的网络，直接与宿主机进行IPC（进程间通信）通信，可以看到宿主机里正在运行的所有进程。 
加入了hostNetwork:true会直接将我们的宿主机的9100端口映射出来，从而不需要创建service 在我们的宿主机上就会有一个9100的端口 
      containers:
      - name: node-exporter
        image: prom/node-exporter:v0.16.0
        ports:
        - containerPort: 9100
        resources:
          requests:
            cpu: 0.15 #这个容器运行至少需要0.15核cpu      
        securityContext:
          privileged: true #开启特权模式         
        args:
        - --path.procfs #配置挂载宿主机（node节点）的路径 
        - /host/proc
        - --path.sysfs #配置挂载宿主机（node节点）的路径 
        - /host/sys
        - --collector.filesystem.ignored-mount-points 
        - '"^/(sys|proc|dev|host|etc)($|/)"'
        #通过正则表达式忽略某些文件系统挂载点的信息收集         
        volumeMounts:
        - name: dev
          mountPath: /host/dev
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: rootfs
          mountPath: /rootfs
          #将主机/dev、/proc、/sys这些目录挂在到容器中，这是因为我们采集的很多节点数据都是通过这些文件来获取系统信息的。
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: dev
          hostPath:
            path: /dev
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
```



\#通过 kubectl apply 更新 node-exporter.yaml 文件

```shell
 [root@xianchaomaster1]# kubectl apply -f node-export.yaml 
```

#查看 node-exporter 是否部署成功

```shell
 [root@xianchaomaster1]# kubectl get pods -n monitor-sa 
```

显示如下，看到 pod 的状态都是 running，说明部署成功
 NAME READY STATUS RESTARTS AGE

 node-exporter-9qpkd 1/1 Running 0 89s

 node-exporter-zqmnk 1/1 Running 0 89s

通过 node-exporter 采集数据
 curl http://主机 ip:9100/metrics

\#node-export 默认的监听端口是 9100，可以看到当前主机获取到的所有监控数据

curl http://192.168.40.180:9100/metrics | grep node_cpu_seconds 显示 192.168.40.180 主机 cpu 的使用情况

\# HELP node_cpu_seconds_total Seconds the cpus spent in each mode. # TYPE node_cpu_seconds_total counter node_cpu_seconds_total{cpu="0",mode="idle"} 72963.37 node_cpu_seconds_total{cpu="0",mode="iowait"} 9.35 node_cpu_seconds_total{cpu="0",mode="irq"} 0 node_cpu_seconds_total{cpu="0",mode="nice"} 0 node_cpu_seconds_total{cpu="0",mode="softirq"} 151.4 node_cpu_seconds_total{cpu="0",mode="steal"} 0 node_cpu_seconds_total{cpu="0",mode="system"} 656.12 node_cpu_seconds_total{cpu="0",mode="user"} 267.1

\#HELP:解释当前指标的含义，上面表示在每种模式下 node 节点的 cpu 花费的时间，以 s 为单位 #TYPE:说明当前指标的数据类型，上面是 counter 类型 node_cpu_seconds_total{cpu="0",mode="idle"} :
 cpu0 上 idle 进程占用 CPU 的总时间，CPU 占用时间是一个只增不减的度量指标，从类型中也可以看

出 node_cpu 的数据类型是 counter(计数器)

counter 计数器:只是采集递增的指标

curl http://192.168.40.180:9100/metrics | grep node_load # HELP node_load1 1m load average.
 \# TYPE node_load1 gauge
 node_load1 0.1

node_load1 该指标反映了当前主机在最近一分钟以内的负载情况，系统的负载情况会随系统资源的 使用而变化，因此 node_load1 反映的是当前状态，数据可能增加也可能减少，从注释中可以看出当前指 标类型为 gauge(标准尺寸)

gauge 标准尺寸:统计的指标可增加可减少

## 10、Prometheus server 安装和配置

### 10.1 创建sa账号，对sa做rbac授权

\#创建一个 sa 账号 monitor

```shell
[root@xianchaomaster1 ~]# kubectl create serviceaccount monitor -n monitor-sa
```

\#把 sa 账号 monitor 通过 clusterrolebing 绑定到 clusterrole 上

```shell
[root@xianchaomaster1 ~]# kubectl create clusterrolebinding monitor-clusterrolebinding -n monitor-sa --clusterrole=cluster-admin --serviceaccount=monitor-sa:monitor
```

### 10.2 创建 prometheus 数据存储目录

\#在 k8s 集群的 xianchaonode1 节点上创建数据存储目录 

```shell
[root@xianchaonode1 ~]# mkdir /data
[root@xianchaonode1 ~]# chmod 777 /data/
```



### 10.3 安装 Prometheus server 服务

####  10.3.1 创建一个 configmap 存储卷，用来存放 prometheus 配置信息

prometheus-cfg.yaml 文件在课件，可上传到 k8s 控制节点 xianchaomaster1 上: #通过 kubectl apply 更新 configmap

```shell
 [root@xianchaomaster1 prometheus]# kubectl apply -f prometheus-cfg.yaml 
```

prometheus-cfg.yaml 文件内容如下:

```yaml
---
kind: ConfigMap
apiVersion: v1
metadata:
labels:
	app: prometheus
	name: prometheus-config
	namespace: monitor-sa
data:
	prometheus.yml: |
		global:
			scrape_interval: 15s #采集目标主机监控据的时间间隔 
			scrape_timeout:10s #数据采集超时时间，默认10s 
			evaluation_interval: 1m #触发告警检测的时间，默认是 1m
		scrape_configs:
			#scrape_configs:配置数据源，称为 target，每个 target 用 job_name 命名。又分为静态配置和服 务发现
		- job_name: 'kubernetes-node'
      kubernetes_sd_configs:#使用的是 k8s 的服务发现
      - role: node# 使用 node 角色，它使用默认的 kubelet 提供的 http 端口来发现集群中每个 node 节点。 
    	relabel_configs:#重新标记
			- source_labels: [__address__] #配置的原始标签，匹配地址
			  regex: '(.*):10250' #匹配带有 10250 端口的 url
			  replacement:'${1}:9100' #把匹配到的ip:10250的ip保留 
			  target_label: __address__ #新生成的 url 是${1}获取到的 ip:9100 
			  action: replace
```

![image-20220524224955519](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524224955519.png)

```yaml
			- action: labelmap
			#匹配到下面正则表达式的标签会被保留,如果不做 regex 正则的话，默认只是会显示 instance 标签
				regex: __meta_kubernetes_node_label_(.+)
```

![image-20220524225232775](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225232775.png)

注意:Before relabeling 表示匹配到的所有标签 instance="xianchaomaster1"
 Before relabeling:

__address__="192.168.40.180:10250" __meta_kubernetes_node_address_Hostname="xianchaomaster1" __meta_kubernetes_node_address_InternalIP="192.168.40.180"

__meta_kubernetes_node_annotation_kubeadm_alpha_kubernetes_io_cri_socket="/var/run/dockersh im.sock"
 __meta_kubernetes_node_annotation_node_alpha_kubernetes_io_ttl="0" __meta_kubernetes_node_annotation_projectcalico_org_IPv4Address="192.168.40.180/24" __meta_kubernetes_node_annotation_projectcalico_org_IPv4IPIPTunnelAddr="10.244.123.64" __meta_kubernetes_node_annotation_volumes_kubernetes_io_controller_managed_attach_detach="t rue"

__meta_kubernetes_node_label_beta_kubernetes_io_arch="amd64" __meta_kubernetes_node_label_beta_kubernetes_io_os="linux" __meta_kubernetes_node_label_kubernetes_io_arch="amd64" __meta_kubernetes_node_label_kubernetes_io_hostname="xianchaomaster1" __meta_kubernetes_node_label_kubernetes_io_os="linux" __meta_kubernetes_node_label_node_role_kubernetes_io_control_plane="" __meta_kubernetes_node_label_node_role_kubernetes_io_master="" __meta_kubernetes_node_name="xianchaomaster1" __metrics_path__="/metrics"

__scheme__="http" instance="xianchaomaster1" job="kubernetes-node"

```yaml
    - job_name: 'kubernetes-node-cadvisor'
    # 抓取 cAdvisor 数据，是获取 kubelet 上/metrics/cadvisor 接口数据来获取容器的资源使用情况
      kubernetes_sd_configs:
      - role:  node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap #把匹配到的标签保留
        regex: __meta_kubernetes_node_label_(.+)
        #保留匹配到的具有__meta_kubernetes_node_label 的标签
      - target_label: __address__
      #获取到的地址:__address__="192.168.40.180:10250"
        replacement: kubernetes.default.svc:443
        #把获取到的地址替换成新的地址 kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        #把原始标签中__meta_kubernetes_node_name 值匹配到
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        #把 metrics 替换成新的值 api/v1/nodes/xianchaomaster1/proxy/metrics/cadvisor${1}是__meta_kubernetes_node_name 获取到的值新的 url 就是https://kubernetes.default.svc:443/api/v1/nodes/xianchaomaster1/proxy/metrics/cadvisor

    - job_name: 'kubernetes-apiserver'
      kubernetes_sd_configs:
      - role: endpoints
      #使用 k8s 中的 endpoint 服务发现，采集 apiserver 6443 端口获取到的数据
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      #endpoint 这个对象的名称空间
      #endpoint 对象的服务名
      #exnpoint 的端口名称
        action: keep #采集满足条件的实例，其他实例不采集
        regex: default;kubernetes;https
        #正则匹配到的默认空间下的 service 名字是 kubernetes，协议是 https 的 endpoint 类型保留下来
    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
        # 重新打标仅抓取到的具有 "prometheus.io/scrape: true" 的 annotation 的端点，意思是说如果 某个 service 具有 prometheus.io/scrape = true annotation 声明则抓取，annotation 本身也是键值结 构，所以这里的源标签设置为键，而 regex 设置值 true，当值匹配到 regex 设定的内容时则执行 keep 动 作也就是保留，其余则丢弃。
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
        #重新设置 scheme，匹配源标签__meta_kubernetes_service_annotation_prometheus_io_scheme 也就是 prometheus.io/scheme annotation，如果源标签的值匹配到 regex，则把值替换为__scheme__对应 的值。
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
        # 应用中自定义暴露的指标，也许你暴露的 API 接口不是/metrics 这个路径，那么你可以在这个 POD 对应的 service 中做一个"prometheus.io/path = /mymetrics" 声明，上面的意思就是把你声明的这 个路径赋值给__metrics_path__，其实就是让 prometheus 来获取自定义应用暴露的 metrices 的具体路 径，不过这里写的要和 service 中做好约定，如果 service 中这样写 prometheus.io/app-metrics- path: '/metrics' 那么你这里就要
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        # 暴露自定义的应用的端口，就是把地址和你在 service 中定义的 "prometheus.io/port = <port>" 声明做一个拼接，然后赋值给__address__，这样 prometheus 就能获取自定义应用的端口，然 后通过这个端口再结合__metrics_path__来获取指标，如果__metrics_path__值不是默认的/metrics 那 么就要使用上面的标签替换来获取真正暴露的具体路径。

      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name 
```





![image-20220524225440741](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225440741.png)



- \#更新 configmap 资源
  
  ```
  [root@xianchaomaster1 prometheus]# kubectl apply -f prometheus-cfg.yaml
  ```

  
  
  #### 10.3.2 通过 deployment 部署 prometheus
  
   安装 prometheus 需要的镜像 prometheus-2-2-1.tar.gz 在课件，上传到 k8s 的工作节点
  
  xianchaonode1 上，手动解压:
  
  ```
   [root@xianchaonode1 ~]# docker load -i prometheus-2-2-1.tar.gz
  ```
  
   prometheus-deploy.yaml 文件在课件里，上传到自己的 k8s 的控制节点 xianchaomaster1 

\#通过 kubectl apply 更新 prometheus

```
 [root@xianchaomaster1]# kubectl apply -f prometheus-deploy.yaml #查看 prometheus 是否部署成功
 [root@xianchaomaster1]# kubectl get pods -n monitor-sa 显示如下，可看到 pod 状态是 running，说明 prometheus 部署成功
```

```
NAME READY   STATUS    RESTARTS   AGE
node-exporter-9qpkd 1/1     Running   0          76m
node-exporter-zqmnk 1/1     Running   0          76m
prometheus-server-85dbc6c7f7-nsg94 1/1 Running 0 6m7
```

注意:在上面的 prometheus-deploy.yaml 文件有个 nodeName 字段，这个就是用来指定创建的这个 prometheus 的 pod 调度到哪个节点上，我们这里让 nodeName=xianchaonode1，也即是让 pod 调度到 xianchaonode1 节点上，因为 xianchaonode1 节点我们创建了数据目录/data，所以大家记住:你在 k8s 集群的哪个节点创建/data，就让 pod 调度到哪个节点，nodeName 根据你们自己环境主机去修改即可。

prometheus-deploy.yaml 文件内容如下:

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-server
  namespace: monitor-sa
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      component: server
    #matchExpressions:
    #- {key: app, operator: In, values: [prometheus]}
    #- {key: component, operator: In, values: [server]}
  template:
    metadata:
      labels:
        app: prometheus
        component: server
      annotations:
        prometheus.io/scrape: 'false'
    spec:
      nodeName: xianchaonode1
      serviceAccountName: monitor
      containers:
      - name: prometheus
        image: prom/prometheus:v2.2.1
        imagePullPolicy: IfNotPresent
        command:
          - prometheus
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus #旧数据存储目录
          - --storage.tsdb.retention=720h  #何时删除旧数据，默认为 15 天。
          - --web.enable-lifecycle #开启热加载
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/prometheus/prometheus.yml
          name: prometheus-config
          subPath: prometheus.yml
        - mountPath: /prometheus/
          name: prometheus-storage-volume
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
            items:
              - key: prometheus.yml
                path: prometheus.yml
                mode: 0644
        - name: prometheus-storage-volume
          hostPath:
           path: /data
           type: Directory
```

#### 10.3.3 给 prometheus pod 创建一个 service

 prometheus-svc.yaml 文件在课件，可上传到 k8s 的控制节点 xianchaomaster1 上:

\#通过 kubectl apply 更新 service

```shell
[root@xianchaomaster1]# kubectl apply -f prometheus-svc.yaml
```

prometheus-svc.yaml 文件内容如下:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitor-sa
  labels:
    app: prometheus
spec:
  type: NodePort
  ports:
    - port: 9090
      targetPort: 9090
      protocol: TCP
  selector:
    app: prometheus
    component: server
```



\#查看 service 在物理机映射的端口

```shell
 [root@xianchaomaster1]# kubectl get svc -n monitor-sa
 显示如下:
 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
 prometheus NodePort 10.96.45.93 <none> 9090:32732/TCP 50s
```

通过上面可以看到 service 在宿主机上映射的端口是 32732，这样我们访问 k8s 集群的 master1 节点 的 ip:32732，就可以访问到 prometheus 的 web ui 界面了

\#访问 prometheus web ui 界面 火狐浏览器输入如下地址: http://192.168.40.180:32732/graph 可看到如下页面:

![image-20220524225509283](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225509283.png)

\#点击页面的 Status->Targets，可看到如下,说明我们配置的服务发现可以正常采集数据

![image-20220524225522573](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225522573.png)

![image-20220524225541685](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225541685.png)

![image-20220524225551918](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225551918.png)

![image-20220524225603859](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225603859.png)

![image-20220524225615793](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225615793.png)



#### 10.3.4 Prometheus热加载

 \#为了每次修改配置文件可以热加载 prometheus，也就是不停止 prometheus，就可以使配置生效，



想要使配置生效可用如下热加载命令:


![image-20220524225629448](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225629448.png)

```shell
 [root@xianchaomaster1 prometheus]# kubectl get pods -n monitor-sa -o wide -l app=prometheus
```

```shell
#10.244.121.4 是 prometheus 的 pod 的 ip 地址，如何查看 prometheus 的 pod ip 想要使配置生效可用如下命令热加载:
[root@xianchaomaster1]# curl -X POST http://10.244.121.4:9090/-/reload
#热加载速度比较慢，可以暴力重启 prometheus，如修改上面的 prometheus-cfg.yaml 文件之后，可 执行如下强制删除:
kubectl delete -f prometheus-cfg.yaml
kubectl delete -f prometheus-deploy.yaml
#然后再通过 apply 更新:
kubectl apply -f prometheus-cfg.yaml
kubectl apply -f prometheus-deploy.yaml 注意: 线上最好热加载，暴力删除可能造成监控数据的丢失
```

## 11、可视化 UI 界面 Grafana 的安装和配置 

### 11.1 Grafana 介绍

Grafana 是一个跨平台的开源的度量分析和可视化工具，可以将采集的数据可视化的展示，并及时通 知给告警接收方。它主要有以下六大特点:

 1、展示方式:快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具 有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式; 2、数据源:Graphite，InfluxDB，OpenTSDB，Prometheus，Elasticsearch，CloudWatch 和 KairosDB 等;

3、通知提醒:以可视方式定义最重要指标的警报规则，Grafana 将不断计算并发送通知，在数据达 到阈值时通过 Slack、PagerDuty 等获得通知;

 4、混合展示:在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数 据源;

 5、注释:使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据 和标记。

### 11.2 安装 Grafana

 安装 Grafana 需要的镜像 heapster-grafana-amd64_v5_0_4.tar.gz,把镜像上传到 k8s 的工作节点 xianchaonode1 上，手动解压:

```shell
 [root@xianchaonode1 ~]# docker load -i heapster-grafana-amd64_v5_0_4.tar.gz 
```

grafana.yaml 文件在课件里，可上传到 k8s 的控制节点:

更新 yaml 文件:

```shell
 [root@xianchaomaster1 prometheus]# kubectl apply -f grafana.yaml
```

\#查看 grafana 是否创建成功:

```shell
[root@xianchaomaster1 prometheus]# kubectl get pods -n kube-system -l task=monitoring
```

显示如下，说明部署成功

```shell
 NAME READY STATUS RESTARTS AGE
 monitoring-grafana-675798bf47-cw9hr 1/1 Running 0 39s
```

grafana.yaml 文件内容如下: 

```yaml
kind: Deployment
metadata:
  name: monitoring-grafana
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      task: monitoring
      k8s-app: grafana
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: grafana
    spec:
      containers:
      - name: grafana
        image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4
        ports:
        - containerPort: 3000
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/ssl/certs
          name: ca-certificates
          readOnly: true
        - mountPath: /var
          name: grafana-storage
        env:
        - name: INFLUXDB_HOST
          value: monitoring-influxdb
        - name: GF_SERVER_HTTP_PORT
          value: "3000"
          # The following env variables are required to make Grafana accessible via
          # the kubernetes api-server proxy. On production clusters, we recommend
          # removing these env variables, setup auth for grafana, and expose the grafana
          # service using a LoadBalancer or a public IP.
        - name: GF_AUTH_BASIC_ENABLED
          value: "false"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Admin
        - name: GF_SERVER_ROOT_URL
          # If you're only using the API Server proxy, set this value instead:
          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
          value: /
      volumes:
      - name: ca-certificates
        hostPath:
          path: /etc/ssl/certs
      - name: grafana-storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: monitoring-grafana
  name: monitoring-grafana
  namespace: kube-system
spec:
  # In a production setup, we recommend accessing Grafana through an external Loadbalancer
  # or through a public IP.
  # type: LoadBalancer
  # You could also use NodePort to expose the service at a randomly-generated port
  # type: NodePort
  ports:
  - port: 80
    targetPort: 3000
  selector:
    k8s-app: grafana
  type: NodePort

```

### 11.3 Grafana 界面接入 Prometheus 数据源

查看 grafana 前端的 service
 [root@xianchaomaster1]# kubectl get svc -n kube-system | grep grafana 显示如下:
 monitoring-grafana NodePort 10.106.3.47 <none> 80:30858/TCP 1)登陆 grafana，在浏览器访问
 192.168.40.180:30858
 可看到如下界面:

![image-20220524225705402](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225705402.png)

2)配置 grafana 界面:

开始配置 grafana 的 web 界面:
 选择 Create your first data source

![image-20220524225713535](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225713535.png)

出现如下

![image-20220524225729043](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225729043.png

Name: Prometheus
 Type: Prometheus
 HTTP 处的 URL 写 如下:

http://prometheus.monitor-sa.svc:9090 配置好的整体页面如下:

![image-20220524225755186](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225755186.png)

点击左下角 Save & Test，出现如下 Data source is working，说明 prometheus 数据源成功的被 grafana 接入了

![image-20220524225810390](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225810390.png)



```
导入的监控模板，可在如下链接搜索
https://grafana.com/dashboards?dataSource=prometheus&search=kubernetes
```

可直接导入 node_exporter.json 监控模板，这个可以把 node 节点指标显示出来

node_exporter.json 在课件里
 可直接导入 docker_rev1.json，显示容器资源指标的，docker_rev1.json 在课件里

```
怎么导入监控模板，按如下步骤:
```

上面 Save & Test 测试没问题之后，就可以返回 Grafana 主页面

![image-20220524225821959](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225821959.png)

点击左侧+号下面的 Import，出现如下界面

![image-20220524225833467](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225833467.png)



选择 Upload json file，出现如下

![image-20220524225843672](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225843672.png)

选择一个本地的 json 文件，我们选择的是上面让大家下载的 node_exporter.json 这个文件，选择之 后出现如下:

![image-20220524225903395](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225903395.png)

注:箭头标注的地方 Name 后面的名字是 node_exporter.json 定义的
** Prometheus 后面需要变成 Prometheus，然后再点击 Import，就可以出现如下界面:

![image-20220524225915244](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225915244.png)

导入 docker_rev1.json 监控模板，步骤和上面导入 node_exporter.json 步骤一样，导入之后显示 如下:

![image-20220524225926857](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225926857.png)

![image-20220524225949323](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524225949323.png)

扩展:如果 Grafana 导入 Prometheusz 之后，发现仪表盘没有数据，如何排查? 1、打开 grafana 界面，找到仪表盘对应无数据的图标

![image-20220524230002519](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230002519.png)

Edit 之后出现如下:

![image-20220524230014927](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230014927.png)

node_cpu_seconds_total 就是 grafana 上采集的 cpu 的时间，需要到 prometheus ui 界面看看采集 的指标是否是 node_cpu_seconds_total

![image-20220524230053808](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230053808.png)



如果在 prometheus ui 界面输入 node_cpu_seconds_total 没有数据，那就看看是不是 prometheus 采集的数据是 node_cpu_seconds_totals，怎么看呢?

![image-20220524230111151](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230111151.png)

## 12、安装 kube-state-metrics 组件

 kube-state-metrics 是什么?
 kube-state-metrics 通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、 Node、Pod，需要注意的是 kube-state-metrics 只是简单的提供一个 metrics 数据，并不会存储这 些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储，主要关注的是业务相关的一 些元数据，比如 Deployment、Pod、副本状态等;调度了多少个 replicas?现在可用的有几个?多 少个 Pod 是 running/stopped/terminated 状态?Pod 重启了多少次?我有多少 job 在运行中。
 安装 kube-state-metrics 组件
 1)创建 sa，并对 sa 授权
 在 k8s 的控制节点生成一个 kube-state-metrics-rbac.yaml 文件，kube-state-metrics-rbac.yaml 文件在课件，大家上传到 k8s 的控制节点即可:
 通过 kubectl apply 更新资源清单 yaml 文件 

 

```shell
[root@xianchaomaster1 prometheus]# kubectl apply -f kube-state-metrics-rbac.yaml
```

kube-state-metrics-rbac.yaml 文件内容如下: 
```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "services", "resourcequotas", "replicationcontrollers", "limitranges", "persistentvolumeclaims", "persistentvolumes", "namespaces", "endpoints"]
  verbs: ["list", "watch"]
- apiGroups: ["extensions"]
  resources: ["daemonsets", "deployments", "replicasets"]
  verbs: ["list", "watch"]
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["list", "watch"]
- apiGroups: ["batch"]
  resources: ["cronjobs", "jobs"]
  verbs: ["list", "watch"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system

```



2)安装 kube-state-metrics 组件

安装 kube-state-metrics 组件需要的镜像在课件，可上传到 k8s 各个工作节点，手动解压:

```shell
 [root@xianchaonode1 ~]# docker load -i kube-state-metrics_1_9_0.tar.gz
```

 在 k8s 的控制节点生成一个 kube-state-metrics-deploy.yaml 文件，kube-state-metrics- deploy.yaml 在课件，可上传到 xianchaomaster1 节点:

通过 kubectl apply 更新 yaml 文件

```shell
[root@xianchaomaster1 prometheus]# kubectl apply -f kube-state-metrics-deploy.yaml
```

kube-state-metrics-deploy.yaml 文件内容如下

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v1.9.0
        ports:
        - containerPort: 8080
```

查看 kube-state-metrics 是否部署成功

```shell
[root@xianchaomaster1]# kubectl get pods -n kube-system -l app=kube-state-metrics
```

显示如下，看到 pod 处于 running 状态，说明部署成功

```shell
kube-state-metrics-79c9686b96-4njrs 1/1 Running 0 76s
```

3)创建 service
 在 8s 的控制节点生成一个 kube-state-metrics-svc.yaml 文件，kube-state-metrics-svc.yaml 文 件在课件，可上传到 k8s 的 xianchaomaster1 节点: 

通过 kubectl apply 更新 yaml

```shell
[root@xianchaomaster1]# kubectl apply -f kube-state-metrics-svc.yaml
```

kube-state-metrics-svc.yaml 文件内容如下: 

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: 'true'
  name: kube-state-metrics
  namespace: kube-system
  labels:
    app: kube-state-metrics
spec:
  ports:
  - name: kube-state-metrics
    port: 8080
    protocol: TCP
  selector:
    app: kube-state-metrics
```

app: kube-state-metrics

查看 service 是否创建成功

```shell
[root@xianchaomaster1]# kubectl get svc -n kube-system | grep kube-state-metrics
```

显示如下，说明创建成功

```shell
kube-state-metrics ClusterIP 10.105.53.102 <none> 8080/TCP
```

在 grafana web 界面导入 Kubernetes Cluster (Prometheus)-1577674936972.json 和 Kubernetes cluster monitoring (via Prometheus) (k8s 1.16)-1577691996738.json，Kubernetes Cluster (Prometheus)-1577674936972.json 和 Kubernetes cluster monitoring (via Prometheus) (k8s 1.16)-1577691996738.json 文件在课件

导入 Kubernetes Cluster (Prometheus)-1577674936972.json 文件

![image-20220524230239424](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230239424.png)



导入之后出现如下页面

![image-20220524230253974](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230253974.png)



在 grafana web 界面导入 Kubernetes cluster monitoring (via Prometheus) (k8s 1.16)- 1577691996738.json

![image-20220524230311487](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230311487.png)



导入之后出现如下页面

![image-20220524230321523](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230321523.png)



## 13、配置 alertmanager-发送报警到 qq 邮箱

报警:指 prometheus 将监测到的异常事件发送给 alertmanager 通知:alertmanager 将报警信息发送到邮件、微信、钉钉等

\#创建 alertmanager 配置文件
 在 k8s 的控制节点创建 alertmanager-cm.yaml 文件，alertmanager-cm.yaml 文件在课件，可上传到 k8s 的 xianchaomaster1 节点

通过 kubectl apply 更新文件

```
    kubectl apply -f alertmanager-cm.yaml
```

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: alertmanager
  namespace: monitor-sa
data:
  alertmanager.yml: |-
    global:
      resolve_timeout: 1m
      smtp_smarthost: 'smtp.163.com:25'
      smtp_from: '15011572657@163.com'
      smtp_auth_username: '15011572657'
      smtp_auth_password: 'BGWHYUOSOOHWEUJM'
      smtp_require_tls: false
    route: #用于配置告警分发策略
      group_by: [alertname] # 采用哪个标签来作为分组依据
      group_wait: 10s # 组告警等待时间。也就是告警产生后等待 10s，如果有同组告警一起发出
      group_interval: 10s # 上下两组发送告警的间隔时间
      repeat_interval: 10m # 重复发送告警的时间，减少相同邮件的发送频率，默认是 1h
      receiver: default-receiver #定义谁来收告警
    receivers:
    - name: 'default-receiver'
      email_configs:
      - to: '1980570647@qq.com'
        send_resolved: true
```





alertmanager 配置文件解释说明:

smtp_smarthost: 'smtp.163.com:25'
 \#163 邮箱的 SMTP 服务器地址+端口
 smtp_from: '15011572657@163.com'
 \#这是指定从哪个邮箱发送报警
 smtp_auth_username: '15011572657'
 \#这是发送邮箱的认证用户，不是邮箱名
 smtp_auth_password: ' BGWHYUOSOOHWEUJM' #这是发送邮箱的授权码而不是登录密码，你们需要用自己的，不要用我的，用我的你会发不出来报 警

    email_configs:
       - to: '1980570647@qq.com'

\#to 后面指定发送到哪个邮箱，我发送到我的 qq 邮箱，大家需要写自己的邮箱地址，不应该跟 smtp_from 的邮箱名字重复

route: #用于设置告警的分发策略 group_by: [alertname]

\#alertmanager 会根据 group_by 配置将 Alert 分组 group_wait: 10s

\# 分组等待时间。也就是告警产生后等待 10s，如果有同组告警一起发出
 group_interval: 10s # 上下两组发送告警的间隔时间
 repeat_interval: 10m # 重复发送告警的时间，减少相同邮件的发送频率，默认是 1h receiver: default-receiver #定义谁来收告警

Prometheus 一条告警的触发流程、等待时间

报警处理流程如下:

1. Prometheus Server 监控目标主机上暴露的 http 接口(这里假设接口 A)，通过 Promethes 配置的 'scrape_interval'定义的时间间隔，定期采集目标主机上监控数据。
2. 当接口 A 不可用的时候，Server 端会持续的尝试从接口中取数据，直到"scrape_timeout"时间后 停止尝试。这时候把接口的状态变为“DOWN”。
3. Prometheus 同时根据配置的"evaluation_interval"的时间间隔，定期(默认 1min)的对 Alert Rule 进行评估;当到达评估周期的时候，发现接口 A 为 DOWN，即 UP=0 为真，激活 Alert，进入 “PENDING”状态，并记录当前 active 的时间;
4. 当下一个 alert rule 的评估周期到来的时候，发现 UP=0 继续为真，然后判断警报 Active 的时间 是否已经超出 rule 里的‘for’ 持续时间，如果未超出，则进入下一个评估周期;如果时间超出， 则 alert 的状态变为“FIRING”;同时调用 Alertmanager 接口，发送相关报警数据。
5. AlertManager 收到报警数据后，会将警报信息进行分组，然后根据 alertmanager 配置的 “group_wait”时间先进行等待。等 wait 时间过后再发送报警信息。
6. 属于同一个 Alert Group 的警报，在等待的过程中可能进入新的 alert，如果之前的报警已经成 功发出，那么间隔“group_interval”的时间间隔后再重新发送报警信息。比如配置的是邮件报警， 那么同属一个 group 的报警信息会汇总在一个邮件里进行发送。
7. 如果 Alert Group 里的警报一直没发生变化并且已经成功发送，等待‘repeat_interval’时间间 隔之后再重复发送相同的报警邮件;如果之前的警报没有成功发送，则相当于触发第 6 条条件，则需 要等待 group_interval 时间间隔后重复发送。

同时最后至于警报信息具体发给谁，满足什么样的条件下指定警报接收人，设置不同报警发送频率， 这里有 alertmanager 的 route 路由规则进行配置。

\#创建 prometheus 和告警规则配置文件

在 k8s 的控制节点生成一个 prometheus-alertmanager-cfg.yaml 文件，prometheus-alertmanager- cfg.yaml 文件在课件，上传到 k8s 的 xianchaomaster1 节点

通过 kubectl apply 更新资源文件

```
    [root@xianchaomaster1]# kubectl delete -f prometheus-cfg.yaml
    [root@xianchaomaster1]# kubectl apply -f prometheus-alertmanager-cfg.yaml
```

注意:prometheus-alertmanager-cfg.yaml 文件大家做实验需要修改，修改内容如下:

```yaml
 - job_name: 'kubernetes-schedule'
          scrape_interval: 5s
 static_configs:
-targets:['192.168.40.180:10251'] #scheduler组件所在节点的ip - job_name: 'kubernetes-controller-manager'
         scrape_interval: 5s
         static_configs:
         - targets: ['192.168.40.180:10252']
- job_name: 'kubernetes-kube-proxy'
scrape_interval: 5s
static_configs:
- targets: ['192.168.40.180:10249','192.168.40.181:10249'] #kube-proxy 组件所在节点的 ip
       - job_name: 'kubernetes-etcd'
         scheme: https
         tls_config:
           ca_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/ca.crt
           cert_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/server.crt
           key_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/server.key
         scrape_interval: 5s
         static_configs:
         - targets: ['192.168.40.180:2379']

```

\#etcd 组件所在节点的 ip

\#安装 prometheus 和 alertmanager
 需要把 alertmanager.tar.gz 镜像包上传的 k8s 的各个工作节点，手动解压:

```
 [root@xianchaonode1 ~]# docker load -i alertmanager.tar.gz
```

在 k8s 的控制节点生成一个 prometheus-alertmanager-deploy.yaml 文件，prometheus- alertmanager-deploy.yaml 文件在课件，可上传到 k8s 的控制节点 xianchaomaster1 上:

注意:配置文件指定了 nodeName: xianchaonode1，这个位置要写你自己环境的 k8s 的 node 节点名字

生成一个 etcd-certs，这个在部署 prometheus 需要

```shell
[root@xianchaomaster1]# kubectl -n monitor-sa create secret generic etcd-certs --from-file=/etc/kubernetes/pki/etcd/server.key--from- file=/etc/kubernetes/pki/etcd/server.crt --from-file=/etc/kubernetes/pki/etcd/ca.crt
```

通过 kubectl apply 更新资源清单 yaml 文件

```shell
[root@xianchaomaster1# kubectl delete -f prometheus-deploy.yaml
[root@xianchaomaster1]# kubectl apply -f prometheus-alertmanager-deploy.yaml
```

 \#查看 prometheus 是否部署成功

```shell
[root@xianchaomaster1]# kubectl get pods -n monitor-sa | grep prometheus #显示如下，说明创建成功:  prometheus-server-6bfc4755f6-cn487 2/2 Running 0 16s
```

\#部署 alertmanager 的 service，方便在浏览器访问

在 k8s 的控制节点生成一个 alertmanager-svc.yaml 文件，alertmanager-svc.yaml 文件在课件里， 可上传到 k8s 的控制节点 xianchaomaster1:

通过 kubectl apply 更新 yaml 文件

```
[root@xianchaomaster1]# kubectl apply -f alertmanager-svc.yaml
```

alertmanager-svc.yaml 文件内容如下: ---

```yaml
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: prometheus
    kubernetes.io/cluster-service: 'true'
  name: alertmanager
  namespace: monitor-sa
spec:
  ports:
  - name: alertmanager
    nodePort: 30066
    port: 9093
    protocol: TCP
    targetPort: 9093
  selector:
    app: prometheus
  sessionAffinity: None
  type: NodePort
```

\#查看 service 在物理机映射的端口

```shell
kubectl get svc -n monitor-sa
```

显示如下:

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE

 alertmanager NodePort 10.101.253.221 <none> 9093:30066/TCP 20s 

prometheus NodePort 10.103.243.87 <none> 9090:32732/TCP 96m

注意:上面可以看到 prometheus 的 service 在物理机映射的端口是 32732，alertmanager 的 service 在物理机映射的端口是 30066
 http://192.168.40.180:30066/#/alerts

![image-20220524230426462](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230426462.png)





访问 prometheus 的 web 界面

点击 status->targets，可看到如下

![image-20220524230438298](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230438298.png)

从上面可以发现 kubernetes-controller-manager 和 kubernetes-schedule 都显示连接不上对应的端 口
 可按如下方法处理:

```shell
 vim /etc/kubernetes/manifests/kube-scheduler.yaml
```

修改如下内容:
 把--bind-address=127.0.0.1 变成--bind-address=192.168.40.180
 把 httpGet:字段下的 hosts 由 127.0.0.1 变成 192.168.40.180
 把—port=0 删除
 \#注意:192.168.40.180 是 k8s 的控制节点 xianchaomaster1 的 ip

```shell
 vim /etc/kubernetes/manifests/kube-controller-manager.yaml 
```

修改之后在 k8s 各个节点执行 systemctl restart kubelet

```shell
kubectl get cs
```

 显示如下:
 NAME STATUS MESSAGE controller-manager Healthy ok
 scheduler Healthy ok
 etcd-0 Healthy {"health":"true"}

ss -antulp | grep :10251
 ss -antulp | grep :10252 可以看到相应的端口已经被物理机监听了 点击 status->targets，可看到如下

![image-20220524230458809](/Users/dgt/Library/Application%20Support/typora-user-images/image-20220524230458809.png)

![](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230512179.png)

ERROR



把--bind-address=127.0.0.1 变成--bind-address=192.168.40.130 把 httpGet:字段下的 hosts 由 127.0.0.1 变成 192.168.40.180 把—port=0 删除

kubernetes-kube-proxy 显示如下:



tcp LISTEN 0 128 [::]:10249 点击 status->targets，可看到如下

![image-20220524230545316](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230545316.png)



是因为 kube-proxy 默认端口 10249 是监听在 127.0.0.1 上的，需要改成监听到物理节点上，按如下 方法修改，线上建议在安装 k8s 的时候就做修改，这样风险小一些:

```shell
 kubectl edit configmap kube-proxy -n kube-system
```

 把 metricsBindAddress 这段修改成 metricsBindAddress: 0.0.0.0:10249

然后重新启动 kube-proxy 这个 pod

```shell
 [root@xianchaomaster1]# kubectl get pods -n kube-system | grep kube-proxy |awk '{print $1}' | xargs kubectl delete pods -n kube-system
 [root@xianchaomaster1]# ss -antulp |grep :10249
```

 可显示如下

点击 Alerts，可看到如下

![image-20220524230603710](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230603710.png)



把 controller-manager 的 cpu 使用率大于 90%展开，可看到如下

![image-20220524230613897](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230613897.png)

FIRING 表示 prometheus 已经将告警发给 alertmanager，在 Alertmanager 中可以看到有一个 alert。
 登录到 alertmanager web 界面
 浏览器输入 192.168.40.180:30066，显示如下

![image-20220524230628448](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230628448.png)



这样我在我的 qq 邮箱，1980570647@qq.com 就可以收到报警了，如下

![image-20220524230640586](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230640586.png)

扩展:暴力更新配置文件
 修改 prometheus 任何一个配置文件之后，可通过 kubectl apply 使配置生效，执行顺序如下: kubectl delete -f alertmanager-cm.yaml

```shell
 kubectl apply -f alertmanager-cm.yaml
 kubectl delete -f prometheus-alertmanager-cfg.yaml
 kubectl apply -f prometheus-alertmanager-cfg.yaml
 kubectl delete -f prometheus-alertmanager-deploy.yaml
 kubectl apply -f prometheus-alertmanager-deploy.yaml
```

## 14、配置 alertmanager-发送报警到钉钉

  打开电脑版钉钉创建机器人

1.创建钉钉机器人

```
  打开电脑版钉钉，创建一个群，创建自定义机器人，按如下步骤创建
    https://ding-doc.dingtalk.com/doc#/serverapi2/qf2nxq
    https://developers.dingtalk.com/document/app/custom-robot-access
```

我创建的机器人如下: 群设置-->智能群助手-->添加机器人-->自定义-->添加

机器人名称:test 接收群组:钉钉报警测试

安全设置: 自定义关键词:cluster1

上面配置好之后点击完成即可，这样就会创建一个 test 的报警机器人，创建机器人成功之后怎么查 看 webhook，按如下:

点击智能群助手，可以看到刚才创建的 test 这个机器人，点击 test，就会进入到 test 机器人的设 置界面
 出现如下内容:
 机器人名称:test

  接受群组:钉钉报警测试
  消息推送:开启

webhook:

    https://oapi.dingtalk.com/robot/send?access_token=8a53475677339a11cec453c608543c3d85ea73
    b330ea70c4b2de96a0839cbb90

安全设置: 自定义关键词:cluster1

2.安装钉钉的 webhook 插件，在 k8s 的控制节点 xianchaomaster1 操作

```shell
    tar zxvf prometheus-webhook-dingtalk-0.3.0.linux-amd64.tar.gz
```

prometheus-webhook-dingtalk-0.3.0.linux-amd64.tar.gz 压缩包所在的百度网盘地址如下:

链接:https://pan.baidu.com/s/1_HtVZsItq2KsYvOlkIP9DQ 提取码:d59o

```shell
    cd prometheus-webhook-dingtalk-0.3.0.linux-amd64
```

启动钉钉报警插件

nohup ./prometheus-webhook-dingtalk --web.listen-address="0.0.0.0:8060" -- ding.profile="cluster1=https://oapi.dingtalk.com/robot/send?access_token=8a53475677339a1 1cec453c608543c3d85ea73b330ea70c4b2de96a0839cbb90" &

对原来的 alertmanager-cm.yaml 文件做备份

```shell
    cp alertmanager-cm.yaml alertmanager-cm.yaml.bak
```

重新生成一个新的 alertmanager-cm.yaml 文件

```yaml
cat >alertmanager-cm.yaml <<EOF
kind: ConfigMap
apiVersion: v1
metadata:
	name: alertmanager
	namespace: monitor-sa
data:
	alertmanager.yml: |-
		global:
			resolve_timeout: 1m
			smtp_smarthost: 'smtp.163.com:25'
      smtp_from: '15011572657@163.com'
      smtp_auth_username: '1501157****'
      smtp_auth_password: ‘BGWHYUOSOOHWEUJM'
      smtp_require_tls: false
		route:
			group_by: [alertname]
			group_wait: 10s
			group_interval: 10s
			repeat_interval: 10m
			receiver: cluster1
		receivers:
		- name: cluster1
			webhook_configs:
			- url: 'http://192.168.40.180:8060/dingtalk/cluster1/send'
				send_resolved: true
EOF
```

修改 prometheus 任何一个配置文件之后，可通过 kubectl apply 使配置生效，执行顺序如下:

```shell
kubectl  delete -f  alertmanager-cm.yaml
kubectl  apply   -f alertmanager-cm.yaml
kubectl  delete -f prometheus-alertmanager-cfg.yaml
kubectl  apply   -f prometheus-alertmanager-cfg.yaml
kubectl  delete  -f  prometheus-alertmanager-deploy.yaml
kubectl  apply   -f  prometheus-alertmanager-deploy.yaml
```

## 15、配置 alertmanager-发送报警到微信

1 注册企业微信

登陆网址: https://work.weixin.qq.com/

找到应用管理，创建应用 应用名字 wechat 创建成功之后显示如下:

![image-20220524230655740](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230655740.png)

```

```



AgentId:1000003 Secret:Ov5SWq_JqrolsOj6dD4Jg9qaMu1TTaDzVTCrXHcjlFs

2.修改 alertmanager-cm.yaml

```yaml
global:
 smtp_smarthost: 'smtp.163.com:25'
 smtp_from: '15011572657@163.com'
 smtp_auth_username: '15011572657'
 smtp_auth_password: 'BGWHYUOSOOHWEUJM'
 smtp_require_tls: false
route:
    group_by: [alertname]
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 3m
    receiver: "prometheus"
receivers:
- name: 'prometheus'
  wechat_configs:
  - corp_id: wwa82df90a693abb15
    to_user: '@all'
    agent_id: 1000003
    api_secret: Ov5SWq_JqrolsOj6dD4Jg9qaMu1TTaDzVTCrXHcjlFs
```

参数说明:
 secret: 企业微信("企业应用"-->"自定应用"[Prometheus]--> "Secret") 

wechat 是本人自创建应用名称
 corp_id: 企业信息("我的企业"--->"CorpID"[在底部])
 agent_id: 企业微信("企业应用"-->"自定应用"[Prometheus]--> "AgentId")
 wechat 是自创建应用名称 #在这创建的应用名字是 wechat，那么在配置 route 时，receiver 也应该 是 Prometheus
 to_user: '@all' :发送报警到所有人

3.配置自定义告警模板
 cat template_wechat.tmpl
 {{ define "wechat.default.message" }}
 {{ range .Alerts }} ========start========== 告警程序:node_exporter
 告警名称:{{ .Labels.alertname }} 故障主机: {{ .Labels.instance }} 告警主题: {{ .Annotations.summary }} 告警信息: {{ .Annotations.description }} ========end==========
 {{ end }}
 {{ end }}

## 16、Prometheus PromQL 语法

 PromQL(Prometheus Query Language)是 Prometheus 自己开发的表达式语言，语言表现力很丰

```
富，内置函数也很多。使用它可以对时序数据进行筛选和聚合。
```

### 16.1 数据类型

 PromQL 表达式计算出来的值有以下几种类型:
 瞬时向量 (Instant vector): 一组时序，每个时序只有一个采样值
 区间向量 (Range vector): 一组时序，每个时序包含一段时间内的多个采样值 标量数据 (Scalar): 一个浮点数
 字符串 (String): 一个字符串，暂时未用

#### 16.1.1 瞬时向量选择器 瞬时向量选择器用来选择一组时序在某个采样点的采样值。

最简单的情况就是指定一个度量指标，选择出所有属于该度量指标的时序的当前采样值。比如下面的表 达式:

apiserver_request_total

![image-20220524230733305](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230733305.png)

```
 
```



可以通过在后面添加用大括号包围起来的一组标签键值对来对时序进行过滤。比如下面的表达式筛 选出了 job 为 kubernetes-apiservers，并且 resource 为 pod 的时序:

apiserver_request_total{job="kubernetes-apiserver",resource="pods"}

匹配标签值时可以是等于，也可以使用正则表达式。总共有下面几种匹配操作符: =:完全相等
 !=: 不相等
 =~: 正则表达式匹配

!~: 正则表达式不匹配

下面的表达式筛选出了 container 是 kube-scheduler 或 kube-proxy 或 kube-apiserver 的时序数据 container_processes{container=~"kube-scheduler|kube-proxy|kube-apiserver"}

#### 16.1.2 区间向量选择器 区间向量选择器类似于瞬时向量选择器，不同的是它选择的是过去一段时间的采样值。

可以通过在

瞬时向量选择器后面添加包含在 [] 里的时长来得到区间向量选择器。比如下面的表达式选出了所有度 量指标为 apiserver_request_total 且 resource 是 pod 的时序在过去 1 分钟的采样值。

apiserver_request_total{job="kubernetes-apiserver",resource="pods"}[1m]

![image-20220524230753327](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230753327.png)

这个不支持 Graph，需要选择 Console，才会看到采集的数据

说明:时长的单位可以是下面几种之一: s:seconds
 m:minutes
 h:hours

d:days w:weeks y:years

#### 16.1.3 偏移向量选择器

 前面介绍的选择器默认都是以当前时间为基准时间，偏移修饰器用来调整基准时间，使其往前偏移 



一段时间。偏移修饰器紧跟在选择器后面，使用 offset 来指定要偏移的量。比如下面的表达式选择度 量名称为 apiserver_request_total 的所有时序在 5 分钟前的采样值。

apiserver_request_total{job="kubernetes-apiserver",resource="pods"} offset 5m

下面的表达式选择 apiserver_request_total 度量指标在 1 周前的这个时间点过去 5 分钟的采样 值。

apiserver_request_total{job="kubernetes-apiserver",resource="pods"} [5m] offset 1w

#### 16.1.4 聚合操作符

 PromQL 的聚合操作符用来将向量里的元素聚合得更少。总共有下面这些聚合操作符: sum:求和
 min:最小值
 max:最大值
 avg:平均值
 stddev:标准差
 stdvar:方差
 count:元素个数
 count_values:等于某值的元素个数
 bottomk:最小的 k 个元素
 topk:最大的 k 个元素
 quantile:分位数

如:
 计算 xianchaomaster1 节点所有容器总计内存 sum(container_memory_usage_bytes{instance=~"xianchaomaster1"})/1024/1024/1024

计算 xianchaomaster1 节点最近 1m 所有容器 cpu 使用率

sum (rate (container_cpu_usage_seconds_total{instance=~"xianchaomaster1"}[1m])) / sum (machine_cpu_cores{ instance =~"xianchaomaster1"}) * 100

计算最近 1m 所有容器 cpu 使用率
 sum (rate (container_cpu_usage_seconds_total{id!="/"}[1m])) by (id) #把 id 会打印出来
 结果如下:

![image-20220524230812725](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230812725.png)

#### 16.1.5 函数

 Prometheus 内置了一些函数来辅助计算，下面介绍一些典型的。
 abs():绝对值
 sqrt():平方根 *exp():指数计算
 ln():自然对数
 ceil():向上取整
 floor():向下取整
 round():四舍五入取整 delta():计算区间向量里每一个时序第一个和最后一个的差值 sort():排序

## 17、Prometheus 监控扩展

1、promethues采集tomcat监控数据 笔记:

https://note.youdao.com/ynoteshare1/index.html?id=0ddfc17eaf7bac94ad4497d7f5356213&type=note

2、promethues采集redis监控数据
 笔记:** **https://note.youdao.com/ynoteshare1/index.html?id=b9f87092ce8859cd583967677ea332df&type=note

3、Prometheus监控

```shell
[root@xianchaomaster1 prometheus]# yum install mysql -y
[root@xianchaomaster1 prometheus]# yum install mariadb -y
tar -xvf mysqld_exporter-0.10.0.linux-amd64.tar.gz
cd mysqld_exporter-0.10.0.linux-amd64
cp -ar mysqld_exporter /usr/local/bin/
chmod +x /usr/local/bin/mysqld_exporter
```

2.登陆mysql为mysql_exporter创建账号并授权
创建数据库用户。

```shell
mysql> CREATE USER 'mysql_exporter'@'localhost' IDENTIFIED BY 'Abcdef123!.';
```

#对mysql_exporter用户授权

```shell
mysql> GRANT PROCESS, REPLICATION CLIENT, SELECT ON \*.\* TO 'mysql_exporter'@'localhost';
```

exit退出mysql

3.创建 mysql配置文件、运行时可免密码连接数据库:

```shell
cd mysqld_exporter-0.10.0.linux-amd64
cat my.cnf
[client] user=mysql_exporter password=Abcdef123!.
```

4.启动 mysql_exporter客户端
```shell
nohup ./mysqld_exporter --config.my-cnf=./my.cnf &
```

mysqld_exporter的监听端口是9104

5.修改prometheus-alertmanager-cfg.yaml文件，添加如下

```yaml
- job_name: 'mysql'**

static_configs:
 - targets: ['192.168.40.180:9104']
```

kubectl apply -f prometheus-alertmanager-cfg.yaml kubectl delete -f prometheus-alertmanager-deploy.yaml kubectl apply -f prometheus-alertmanager-deploy.yaml

grafana 导入 mysql 监控图表

mysql-overview_rev5.json

4、Prometheus监控Nginx
笔记:https://note.youdao.com/ynoteshare1/index.html?id=bea7b4b8f9a78db1679e1ac2ab7**47da5&type=note

5、prometheus监控mongodb
笔记:https://note.youdao.com/ynoteshare1/index.html?id=39b54acb1fbc0199f966115ce952**3bb6&type=note

## 18、Pushgateway
Pushgateway简介:
Pushgateway是prometheus的一个组件，prometheus server*默认是通过exporter主动获取

数据(默认采取pull拉取数据)，pushgateway则是通过被动方式推送数据到 prometheus server， 用户可以写一些自定义的监控脚本把需要监控的数据发送给 pushgateway， 然后pushgateway再把 数据发送给Prometheus server

Pushgateway优点:
Prometheus默认采用定时pull模式拉取argets数据，但是如果不在一个子网或者防火墙，

prometheus就拉取不到targets数据，所以可以采用各个target往 pushgateway上push数据，然 后** prometheus去pushgateway上定时pull数据

在监控业务数据的时候，需要将不同数据汇总,汇总之后的数据可以由 pushgateway统一收集，然 后由Prometheus统一拉取。

pushgateway缺点:
Prometheus拉取状态只针对pushgateway,不能对每个节点都有效;
Pushgateway出现问题，整个采集到的数据都会出现问题
 监控下线，prometheus还会拉取到旧的监控数据，需要手动清理pushgateway不要的数据。

安装pushgateway，在k8s-node节点(192.168.40.181)操作:
在k8s-node

```
[root@xianchaonode1 ~]# docker load -i pushgateway.tar.gz [root@xianchaonode1 ~]# docker run -d --name pushgateway -p 9091:9091 prom/pushgateway
```

在浏览器访问192.168.40.181:9091出现如下ui界面

![image-20220524230850420](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230850420.png)




推送指定的数据格式到pushgateway

向{job="test_job"}添加单条数据:

echo " metric 3.6" | curl --data-binary @-

http://192.168.40.181:9091/metrics/job/test_job**

注:--data-binary表示发送二进制数据，注意:它是使用POST方式发送的!

添加复杂数据

```shell
cat <<EOF | curl --data-binary @- http://192.168.40.181:9091/metrics/job/test_job/instance/test_instance
#TYPE node_memory_usage gauge node_memory_usage 36
# TYPE memory_total gauge node_memory_total 36000**
EOF
```

删除某个组下某个实例的所有数据


```shell
curl -X DELETE http://192.168.40.181:9091/metrics/job/test_job/instance/test_instance
```

删除某个组下的所有数据:

```shell
curl -X DELETE http://192.168.40.181:9091/metrics/job/test_job
```

把数据上报到pushgateway

在被监控服务所在的机器配置数据上报想要把192.168.40.181这个机器的内存数据上报到** pushgateway，下面步骤需要在192.168.40.181操作

```shell
cat push.sh
node_memory_usages=$(free -m | grep Mem | awk '{print $3/$2\*100}') job_name="memory"
instance_name="192.168.40.181"
cat <<EOF | curl --data-binary @-**
http://192.168.40.181:9091/metrics/job/$job_name/instance/$instance_name #TYPE node_memory_usages gauge
node_memory_usages $node_memory_usages EOF**
sh push.sh
```
打开pushgateway web ui界面，可看到如下:*

![image-20220524230922170](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230922170.png)

打开prometheus ui界面，可看到如下node_memory_usages的metrics指标

![image-20220524230932146](https://raw.githubusercontent.com/gtdong/images/main/blogimage-20220524230932146.png)

设置计划任务，定时上报数据

```shell
chmod +x push.sh**
crontab -e
*/1 * * * * /usr/bin/bash /root/push.sh
```

注意:从上面配置可以看到，我们上传到pushgateway中的数据有job有instance，而 prometheus配置pushgateway这个ob_name中也有job和instance，这个job和instance是 指pushgateway实例本身，添加honor_labels: true参数， 可以避免promethues的targets列 表中的job_name是pushgateway的job、nstance和上报到pushgateway数据的job和nstance冲突。

